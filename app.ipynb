{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "WUGscpmifELh",
    "outputId": "aac3153e-f5db-4e2e-e1c8-add12fec36c2"
   },
   "outputs": [],
   "source": [
    "# !pip install -q gradio opencv-python facenet-pytorch albumentations numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0BKdj1j2g47E",
    "outputId": "1b9c77c0-5e0f-43e2-9b09-5d5f55337737"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade numpy pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLNhBL36fr_Z"
   },
   "source": [
    "# Setting up UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TDAuGVw9fQHi"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FaMefG9gfQo3"
   },
   "outputs": [],
   "source": [
    "def add_face():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    for _ in range(5):\n",
    "        ret, temp_frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "    frames = []\n",
    "    start_time = time.time()\n",
    "    duration = 6\n",
    "    interval = 0.5\n",
    "    while time.time() - start_time < duration:\n",
    "        ret, frame = cap.read()\n",
    "        if ret :\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "            time.sleep(interval)\n",
    "    cap.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "stiOJBUSfQr_"
   },
   "outputs": [],
   "source": [
    "def unlock_face():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    for _ in range(5):\n",
    "        ret, temp_frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    cap.release()\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face allignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q facenet-pytorch albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(image_size = 160 , margin = 0 , min_face_size = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_face(image):\n",
    "    pil_image = Image.fromarray(image)\n",
    "    alligned_face = mtcnn(pil_image)\n",
    "    if alligned_face is None:\n",
    "        print(\"No face detected\")\n",
    "        return None\n",
    "    return alligned_face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_face(image):\n",
    "    transforms = {\n",
    "        \"rotations\" : A.Rotate(limit = 15 , p = 1.0),\n",
    "        \"brightness\" : A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1.0),\n",
    "        \"blur\" : A.Blur(blur_limit=3, p=1.0),\n",
    "        \"shadow\" : A.RandomGamma(gamma_limit=(80, 120), p=1.0)\n",
    "    }\n",
    "\n",
    "    augmented_images = []\n",
    "\n",
    "    for name, transform in transforms.items():\n",
    "        augmented = transform(image = image)\n",
    "        augmented_images.append(augmented[\"image\"])\n",
    "\n",
    "    return augmented_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FaceNet embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import InceptionResnetV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = InceptionResnetV1(pretrained = 'vggface2').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "def get_facenet_embedding(aligned_face):\n",
    "    \n",
    "    if aligned_face is None:\n",
    "        return None\n",
    "\n",
    "    \n",
    "    if isinstance(aligned_face, torch.Tensor):\n",
    "        aligned_face = transforms.ToPILImage()(aligned_face)\n",
    "\n",
    "    \n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((160, 160)),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) \n",
    "    ])\n",
    "    \n",
    "    face_tensor = preprocess(aligned_face).unsqueeze(0)\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = resnet(face_tensor)\n",
    "        embedding = embedding / embedding.norm(p=2)  \n",
    "\n",
    "    return embedding.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding new face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enroll_pipeline(captured_images):\n",
    "    enrollment_embeddings = []\n",
    "    \n",
    "    for img in captured_images:\n",
    "\n",
    "        aligned_face = align_face(img)\n",
    "        if aligned_face is None:\n",
    "            continue\n",
    "        \n",
    "        if isinstance(aligned_face, torch.Tensor):\n",
    "            aligned_face = transforms.ToPILImage()(aligned_face.cpu())\n",
    "        \n",
    "\n",
    "        embedding = get_facenet_embedding(aligned_face)\n",
    "        if embedding is not None:\n",
    "            enrollment_embeddings.append(embedding)\n",
    "        \n",
    "        \n",
    "        aligned_np = np.array(aligned_face)\n",
    "        if aligned_np.dtype != np.uint8:\n",
    "            \n",
    "            aligned_np = (aligned_np * 255).astype(np.uint8)\n",
    "        \n",
    "        augmented_images = augment_face(aligned_np)\n",
    "        \n",
    "        \n",
    "        for aug_img in augmented_images:\n",
    "            \n",
    "            aug_img = np.clip(aug_img, 0, 255).astype(np.uint8)\n",
    "            try:\n",
    "                pil_aug_img = Image.fromarray(aug_img)\n",
    "            except Exception as e:\n",
    "                print(\"Error converting augmented image to PIL Image:\", e)\n",
    "                continue\n",
    "            aug_embedding = get_facenet_embedding(pil_aug_img)\n",
    "            if aug_embedding is not None:\n",
    "                enrollment_embeddings.append(aug_embedding)\n",
    "    \n",
    "    return enrollment_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlocking with face (Authentication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(emb1 , emb2):\n",
    "    return np.dot(emb1, emb2.T) / (np.linalg.norm(emb1)*np.linalg.norm(emb2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate(enrollment_embeddings, live_image, threshold = 0.8):\n",
    "    \n",
    "    aligned_live = align_face(live_image)\n",
    "    \n",
    "    if aligned_live is None:\n",
    "        print(\"No face detected in live image.\")\n",
    "        return False\n",
    "        \n",
    "    live_embeddings = get_facenet_embedding(aligned_live)\n",
    "    if live_embeddings is None:\n",
    "        return False\n",
    "\n",
    "    for emb in enrollment_embeddings:\n",
    "        similarity = cosine_similarity(live_embeddings.flatten(), emb.flatten())\n",
    "        if similarity >= threshold:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAAAmf_IhvrN"
   },
   "source": [
    "# Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "enrollment_embeddings = None\n",
    "\n",
    "def enrollment_mode():\n",
    "    global enrollment_embeddings\n",
    "    \n",
    "    captured_images = add_face()\n",
    "    \n",
    "    enrollment_embeddings = enroll_pipeline(captured_images)\n",
    "    \n",
    "\n",
    "def authentication_mode():\n",
    "   \n",
    "    live_image = unlock_face()\n",
    "    \n",
    "    if enrollment_embeddings is None:\n",
    "        return \"No enrollment data available. Please enroll a face first.\"\n",
    "    \n",
    "    match_found = authenticate(enrollment_embeddings, live_image, threshold=0.8)\n",
    "    \n",
    "    if match_found:\n",
    "        return \"Unlocked! Face authenticated.\" \n",
    "    else:\n",
    "        return \"Authentication failed. Face not recognized.\"\n",
    "\n",
    "def face_recognition_system(mode):\n",
    "\n",
    "    if mode == \"Enroll Face\":\n",
    "        return enrollment_mode()\n",
    "    elif mode == \"Unlock Face\":\n",
    "        return authentication_mode()\n",
    "    else:\n",
    "        return \"Invalid mode selected.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_css = \"\"\"\n",
    ".gradio-container {\n",
    "  background-color: #000000 !important;\n",
    "  color: #ffffff !important; /* Default text color: white */\n",
    "}\n",
    "\n",
    ".gr-button, button {\n",
    "  background: linear-gradient(45deg, #FFA500, #FFFF00) !important;\n",
    "  color: #000000 !important; /* Black text on the gradient */\n",
    "  border: none !important;\n",
    "  font-weight: 500 !important;\n",
    "  cursor: pointer !important;\n",
    "}\n",
    "\n",
    "h1, h2, .title, .description, label {\n",
    "  color: #FFA500 !important;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_ui():\n",
    "    return gr.update(value=None), \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(css=custom_css) as demo:\n",
    "    gr.Markdown(\"# Face Recognition System\")\n",
    "    gr.Markdown(\n",
    "        \"Select 'Enroll Face' to capture and process images to enroll your face. \"\n",
    "        \"Select 'Unlock Face' to authenticate.\"\n",
    "    )\n",
    "    \n",
    "    mode = gr.Radio([\"Enroll Face\", \"Unlock Face\"], label=\"Select Mode\")\n",
    "    \n",
    "\n",
    "    with gr.Row():\n",
    "        submit_btn = gr.Button(\"Submit\")\n",
    "        clear_btn = gr.Button(\"Clear\")\n",
    "    \n",
    "    \n",
    "    output = gr.Textbox(label=\"Output\")\n",
    "    \n",
    "    \n",
    "    submit_btn.click(fn=face_recognition_system, inputs=mode, outputs=output)\n",
    "    clear_btn.click(fn=clear_ui, inputs=None, outputs=[mode, output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "8Dmgnzt4fQ3M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@10.932] global /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_11nitadzeg/croot/opencv-suite_1691620374638/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n",
      "[ WARN:0@25.072] global /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_11nitadzeg/croot/opencv-suite_1691620374638/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n",
      "[ WARN:0@57.564] global /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_11nitadzeg/croot/opencv-suite_1691620374638/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n",
      "[ WARN:0@72.319] global /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_11nitadzeg/croot/opencv-suite_1691620374638/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n"
     ]
    }
   ],
   "source": [
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJCXoyrEfQ5g"
   },
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
